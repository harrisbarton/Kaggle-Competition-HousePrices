{"cells":[{"metadata":{},"cell_type":"markdown","source":"Things to do:\n*     Add NN, Knn, CatBoost, and other models to the blend.\n*     Better preprocessing of the data - remove outliers, better take care of categorical features, remove NAs, feature  engineering, scaling values differently\n*     Implement a linear regression model to stack oof values\n    "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport random\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport gc\nfrom math import sqrt, log\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import KFold, cross_val_score\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Load in training and test set\ntrain=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.\ntrain[\"SalePrice\"]=np.log1p(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=train[\"SalePrice\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Outliers that I found from another notebook\noutliers = [30, 88, 462, 631, 1322]\ntrain = train.drop(train.index[outliers])\ntrain.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"di={\"1Story\":1,\"2Story\":2,\"1.5Fin\":1.5,\"1.5Unf\":1.5,\"2.5Unf\":2.5,\"2.5Fin\":2.5}\ndata[\"Story\"]=data['HouseStyle'].map(di)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"data[\"Story2\"]=0\ndata.loc[data[\"MSSubClass\"].isin([20,30,40,120]),\"Story2\"]=1\ndata.loc[data[\"MSSubClass\"].isin([45,50,150]),\"Story2\"]=1.5\ndata.loc[data[\"MSSubClass\"].isin([60,70,160]),\"Story2\"]=2\ndata.loc[data[\"MSSubClass\"].isin([75]),\"Story2\"]=2.5"},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature engineering/transforming data into something that can be put into a model using labelencoding or keeping\n#the feature as a category\ntest[\"SalePrice\"]=0\ndata=pd.concat([train,test])\n\n\ndata[\"LowDenseOrFV\"]=0\ndata.loc[data[\"MSZoning\"].isin([\"FV\",\"RL\"]),\"LowDenseOrFV\"]=1\n\ndata[\"StreetToYard\"]=data[\"LotFrontage\"]/data[\"LotArea\"]\n\ndata[\"gravel\"]=0\ndata.loc[data[\"Street\"]==\"Grvl\",\"gravel\"]+=1\ndata.loc[data[\"Alley\"]==\"Grvl\",\"gravel\"]+=1\n\n\nvalue_count=train[\"Neighborhood\"].value_counts()/len(train)\nvalue=value_count.index\nfor i,v in enumerate(value):\n    data.loc[data['Neighborhood']==v,\"NeighborhoodFreq\"]=value_count[i]\n\nfrequencyEncode=[\"Electrical\",\"Heating\",\"Foundation\",\"MasVnrType\",\"Exterior2nd\",\"Exterior1st\",\"RoofStyle\",\"HouseStyle\",\"BldgType\",\"Condition1\",\"LotConfig\",\"LotShape\",\"MSZoning\",\"MSSubClass\"]\nfor feature in frequencyEncode:\n    value_count=train[feature].value_counts()/len(train)\n    value=value_count.index\n    for i,v in enumerate(value):\n        data.loc[data[feature]==v,feature+\"Freq\"]=value_count[v]\n    \n    \nvalue_count=train[\"RoofMatl\"].value_counts()/len(train)\nvalue=value_count.index\nfor i,v in enumerate(value):\n    data.loc[data['RoofMatl']==v,\"RoofMatlFreq\"]=value_count[i]\n\n#di={\"Norm\":\"Norm\",\"Artery\":\"Artery\",\"Feedr\":\"Feedr\",\"RRAn\":\"RR\",\"RRAe\":\"RR\",\"RRNn\":\"RR\",\"RRNe\":\"RR\",\"PosA\":\"Pos\",\"PosN\":\"Pos\"}\n#data['Condition1']=data['Condition1'].map(di)\n#data['Condition2']=data['Condition2'].map(di)\ndi={\"1Story\":1,\"2Story\":2,\"1.5Fin\":1.5,\"SLvl\":1.2,\"SFoyer\":1.1,\"1.5Unf\":1.5,\"2.5Unf\":2.5,\"2.5Fin\":2.5}\ndata[\"story\"]=data['HouseStyle'].map(di)\ndi={\"1Story\":0,\"2Story\":0,\"1.5Fin\":1,\"SLvl\":0,\"SFoyer\":0,\"1.5Unf\":1,\"2.5Unf\":1,\"2.5Fin\":0}\ndata[\"Unfinished\"]=data['HouseStyle'].map(di)\n\ndata.loc[data[\"Exterior2nd\"]=='Wd Shng',\"Exterior2nd\"]=\"Wd Sdng\"\ndata.loc[data[\"Exterior2nd\"]=='CmentBd',\"Exterior2nd\"]=\"CemntBd\"\ndata[\"DifferentExterior\"]=0\ndata.loc[data['Exterior1st']!=data['Exterior2nd'],\"DifferentExterior\"]=1\ndata[\"ExteriorComb\"]=data['Exterior1st']+data['Exterior2nd']\n\nimport math\ndata['HasGarage']=data['GarageQual'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['LotFrontNa']=data['LotFrontage'].apply((lambda x: 1 if math.isnan(x) else 0))\ndata['AlleyNa']=data['Alley'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['VnrNa']=data['MasVnrType'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['BsmtNa']=data['BsmtQual'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['FireplaceNa']=data['FireplaceQu'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['GarageNa']=data['GarageQual'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['PoolNa']=data['PoolQC'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['FenceNa']=data['Fence'].apply((lambda x: 1 if type(x)!=str else 0))\ndata['MiscNa']=data['MiscFeature'].apply((lambda x: 1 if type(x)!=str else 0))\n\n#noneNa=[\"Alley\",\"GarageType\"]\n#for feat in noneNa:\n#    data[feat].fillna(\"None\",inplace=True)\n    \nlabelEncode=[\"MSSubClass\",\"LotConfig\",\"LandContour\",\"MSZoning\",\"RoofMatl\",\"Neighborhood\",\"Condition1\",\"Condition2\",\n             \"BldgType\",'RoofStyle',\"RoofMatl\",\"Exterior1st\",\"Exterior2nd\",\"ExteriorComb\",\"MasVnrType\",\n            \"Foundation\",\"Heating\",\"GarageType\",\"SaleType\",\"SaleCondition\",\"Electrical\",\"HouseStyle\",\n            \"Street\",\"Alley\",\"LandContour\",\"LotConfig\"]\n#for feature in labelEncode:\n#    le = LabelEncoder()\n#    data[feature].fillna('NaN',inplace=True)\n#    le.fit(data[feature])\n#    data[feature]=le.transform(data[feature])\nfor c in labelEncode:\n    data[c] = data[c].astype('category')\n\n\nQualityScale={\"Reg\":3,\"IR1\":2,\"IR2\":1,\"IR3\":0}\ndata['LotShape']=data['LotShape'].map(QualityScale)\n\nQualityScale={\"Gtl\":2,\"Mod\":1,\"Sev\":0}\ndata['LandSlope']=data['LandSlope'].map(QualityScale)\n\nQualityScale={\"Ex\":4,\"Gd\":3,\"TA\":2,\"Fa\":1,\"Po\":0}\ndata['ExterQual']=data['ExterQual'].map(QualityScale)\ndata['ExterCond']=data['ExterCond'].map(QualityScale)\n\nQualityScale={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}\ndata['BsmtQual']=data['BsmtQual'].map(QualityScale)\ndata['BsmtQual'].fillna(0,inplace=True)\ndata['BsmtCond']=data['BsmtCond'].map(QualityScale)\ndata['BsmtCond'].fillna(0,inplace=True)\n\nQualityScale={\"Gd\":4,\"Av\":3,\"Mn\":2,\"No\":1}\ndata['BsmtExposure']=data['BsmtExposure'].map(QualityScale)\ndata['BsmtExposure'].fillna(0,inplace=True)\n\nQualityScale={\"GLQ\":6,\"ALQ\":5,\"BLQ\":4,\"Rec\":3,\"LwQ\":2,\"Unf\":1}\ndata['BsmtFinType1']=data['BsmtFinType1'].map(QualityScale)\ndata['BsmtFinType1'].fillna(0,inplace=True)\ndata['BsmtFinType2']=data['BsmtFinType2'].map(QualityScale)\ndata['BsmtFinType2'].fillna(0,inplace=True)\n\n\n\nQualityScale={\"Ex\":4,\"Gd\":3,\"TA\":2,\"Fa\":1,\"Po\":0}\ndata['HeatingQC']=data['HeatingQC'].map(QualityScale)\n\nQualityScale={\"Y\":1,\"N\":0}\ndata['CentralAir']=data['CentralAir'].map(QualityScale)\n\n\nQualityScale={\"Ex\":4,\"Gd\":3,\"TA\":2,\"Fa\":1,\"Po\":0}\ndata['KitchenQual']=data['KitchenQual'].map(QualityScale)\n\ndi={\"Typ\":1,\"Min1\":2,\"Min2\":3,\"Mod\":4,\"Maj1\":5,\"Maj2\":6,\"Sev\":7,\"Sal\":8}\ndata[\"Functional\"]=data['Functional'].map(di)\n\n#di={\"1Story\":1,\"SLvl\":2,\"SFoyer\":3,\"1.5Fin\":4,\"1.5Unf\":5,\"2Story\":6,\"2.5Unf\":7,\"2.5Fin\":8}\n#data[\"HouseStyle\"]=data['HouseStyle'].map(di)\n\nQualityScale={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}\ndata['FireplaceQu']=data['FireplaceQu'].map(QualityScale)\ndata['FireplaceQu'].fillna(0,inplace=True)\n\nQualityScale={\"Fin\":3,\"RFn\":2,\"Unf\":1}\ndata['GarageFinish']=data['GarageFinish'].map(QualityScale)\ndata['GarageFinish'].fillna(0,inplace=True)\n\nQualityScale={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}\ndata['GarageQual']=data['GarageQual'].map(QualityScale)\ndata['GarageQual'].fillna(0,inplace=True)\n\n\nQualityScale={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}\ndata['GarageCond']=data['GarageCond'].map(QualityScale)\ndata['GarageCond'].fillna(0,inplace=True)\n\nQualityScale={\"Y\":2,\"P\":1,\"N\":0}\ndata['PavedDrive']=data['PavedDrive'].map(QualityScale)\n\nQualityScale={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}\ndata['PoolQC']=data['PoolQC'].map(QualityScale)\ndata['PoolQC'].fillna(0,inplace=True)\n\nQualityScale={\"GdPrv\":4,\"MnPrv\":3,\"GdWo\":2,\"MnWw\":1}\ndata['Fence']=data['Fence'].map(QualityScale)\ndata['Fence'].fillna(0,inplace=True)\n\n\n#oneHotEncode=[\"Exterior1st\",\"SaleCondition\"]\n#data=pd.get_dummies(data,prefix=oneHotEncode, columns = oneHotEncode, drop_first=False)\n#oneHotFeatures=list(data.columns[88:])\ndata[\"TimeElapsed\"]=data['YearRemodAdd']-data[\"YearBuilt\"]\n\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n\nfeatures=['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n       'YearRemodAdd', 'MasVnrArea', 'ExterQual', 'ExterCond', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC',\n       '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n       'BsmtFullBath', 'BsmtHalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n       'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces',\n       'FireplaceQu', 'GarageYrBlt', 'GarageFinish', 'GarageCars',\n       'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'YrSold', 'StreetToYard',\n       'NeighborhoodFreq', 'ElectricalFreq',\n       'TotalSF','LandSlope',\"LotShape\"]\n\nfrom scipy.stats import norm, skew \nnumeric_feats = features\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.Series(skewed_feats)\nskewness.head(10)\n\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    data[feat] = boxcox1p(data[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define code for LIGHTGBM MODEL also define which features will be used. \nfeatures=[\"Fence\",\"HalfBath\",\"FullBath\",\"Heating\",\"RoofMatl\",\"LandSlope\",\"LotConfig\",\"LandContour\",\"Alley\",\"Street\",\"YearBuilt\",\"OverallQual\",\"OverallCond\",\"MSSubClass\",'LowDenseOrFV',\"LotFrontage\",\n          \"LotArea\",\"StreetToYard\",\"gravel\",\"LotShape\",\"NeighborhoodFreq\",\"Neighborhood\",\"Condition1\",\"Condition2\",\n          \"BldgType\",\"HouseStyle\",\"Unfinished\",\"YearRemodAdd\",'RoofStyle',\"Exterior1st\",\"Exterior2nd\",\"DifferentExterior\",\n          \"MasVnrType\",\"MasVnrArea\",\"ExterQual\",\"ExterCond\",\"Foundation\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\n          'BsmtFinSF1',\"BsmtFinType2\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",'HeatingQC',\"CentralAir\",\"Electrical\",\n         \"1stFlrSF\",\"2ndFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\n         'BedroomAbvGr', 'KitchenAbvGr',\"KitchenQual\",\"TotRmsAbvGrd\",\"Functional\",\"Fireplaces\",\"FireplaceQu\",\"GarageType\",\"GarageYrBlt\",\n         \"GarageFinish\",'GarageCars',\"GarageArea\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\n         \"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"PoolArea\",\"YrSold\",\"SaleType\",\"SaleCondition\",\"PoolQC\",\"TotalSF\",\"ElectricalFreq\"]\n#         'ElectricalFreq', 'HeatingFreq', 'FoundationFreq', 'MasVnrTypeFreq', 'Exterior2ndFreq', 'Exterior1stFreq', 'RoofStyleFreq', \n#          'HouseStyleFreq', 'BldgTypeFreq', 'Condition1Freq', 'LotConfigFreq', 'LotShapeFreq', 'MSZoningFreq', 'MSSubClassFreq']#+oneHotFeatures\n#\"HasGarage\",\"LotFrontNa\",\"AlleyNa\",\"VnrNa\",\"BsmtNa\",\"FireplaceNa\",'GarageNa',\"PoolNa\",\"FenceNa\",\"MiscNa\"\n#features=featuresOld[:]\n#for feat in featuresOld:\n#    if feat in oneHotEncode:\n#        features.remove(feat)\n\nimport lightgbm as lgb\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\ndef make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=2):\n    \n    folds = KFold(n_splits=NFOLDS)\n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n    \n    tt_df = tt_df[['Id',target]]    \n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros(len(tr_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 1000,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p/NFOLDS\n        \n        oof_preds = estimator.predict(vl_x)\n        oof[val_idx] = oof_preds\n\n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    print('OOF RMSE:', sqrt(metrics.mean_squared_error(y,oof)))\n    #print('OOF RMSE:', sqrt(metrics.mean_squared_error(y.apply(lambda x: log(x)),np.log(oof))))\n    if LOCAL_TEST:\n        print('Holdout RMSE:', sqrt(metrics.mean_squared_error(tt_df[TARGET], tt_df['prediction'])))\n    \n    return (predictions,oof, estimator)\n\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = False\ntrainIndex=train.index\ntestIndex=list(range(len(trainIndex),len(data)))\nlgb_params = {\n                    'objective':'regression',\n                    'boosting_type':'gbdt',\n                    'metric':'mae',\n                    'n_jobs':-1,\n                    'learning_rate':0.01,\n                    #'num_leaves': 5,\n                    'num_leaves': 10,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.5,\n                    'subsample_freq':1,\n                    'subsample':0.7,\n                    'n_estimators':800,\n                    'max_bin':200,\n                    'verbose':-1,\n                    'seed': SEED,\n                    'early_stopping_rounds':500, \n                } ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_params['learning_rate'] = 0.01\nlgb_params['n_estimators'] = 10000\nlgb_params['early_stopping_rounds'] = 500\ntest_predictions_lgbm,oof_predictions_lgbm,clf = make_predictions(data.iloc[trainIndex,:], data.iloc[testIndex,:], features, \"SalePrice\", lgb_params, NFOLDS=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=[\"Fence\",\"HalfBath\",\"FullBath\",\"Heating\",\"RoofMatl\",\"LandSlope\",\"LotConfig\",\"LandContour\",\"Alley\",\"Street\",\"YearBuilt\",\"OverallQual\",\"OverallCond\",\"MSSubClass\",'LowDenseOrFV',\"LotFrontage\",\n          \"LotArea\",\"StreetToYard\",\"gravel\",\"LotShape\",\"NeighborhoodFreq\",\"Neighborhood\",\"Condition1\",\"Condition2\",\n          \"BldgType\",\"HouseStyle\",\"Unfinished\",\"YearRemodAdd\",'RoofStyle',\"Exterior1st\",\"Exterior2nd\",\"DifferentExterior\",\n          \"MasVnrType\",\"MasVnrArea\",\"ExterQual\",\"ExterCond\",\"Foundation\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\n          'BsmtFinSF1',\"BsmtFinType2\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",'HeatingQC',\"CentralAir\",\"Electrical\",\n         \"1stFlrSF\",\"2ndFlrSF\",\"LowQualFinSF\",\"GrLivArea\",\"BsmtFullBath\",\"BsmtHalfBath\",\n         'BedroomAbvGr', 'KitchenAbvGr',\"KitchenQual\",\"TotRmsAbvGrd\",\"Functional\",\"Fireplaces\",\"FireplaceQu\",\"GarageType\",\"GarageYrBlt\",\n         \"GarageFinish\",'GarageCars',\"GarageArea\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\n         \"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"PoolArea\",\"YrSold\",\"SaleType\",\"SaleCondition\",\"PoolQC\",\"TotalSF\",\"ElectricalFreq\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost can't use category data.  Have to one hot encode categories that were used in the lightgbm model\noneHotEncode=[\"Street\",\"Alley\",\n              \"MSSubClass\",\"LotConfig\",\"LandContour\",\"LandSlope\",\"MSZoning\",\"RoofMatl\",\"Neighborhood\",\"Condition1\",\"Condition2\",\n             \"BldgType\",'RoofStyle',\"Exterior1st\",\"Exterior2nd\",\"ExteriorComb\",\"MasVnrType\",\n            \"Foundation\",\"Heating\",\"GarageType\",\"SaleType\",\"SaleCondition\",\"Electrical\",\"LotShape\",\"HouseStyle\"]\n#newFeatures=[\"ExterQual\",'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\"HeatingQC\",\"CentralAir\",\"KitchenQual\",\"Functional\",\"HouseStyle\",\"FireplaceQu\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PavedDrive\",\"PoolQC\"]\n#newFeatures=['YrSold','Fence', 'MiscFeature', 'MoSold']\noneHotEncode=oneHotEncode#+newFeatures\nstart=len(data.columns)-len(oneHotEncode)\ndata=pd.get_dummies(data,prefix=oneHotEncode, columns = oneHotEncode, drop_first=True)\noneHotFeatures=list(data.columns[start:])\nfeaturesOld=features[:]\nfor feat in featuresOld:\n    if feat in oneHotEncode:\n        features.remove(feat)\nfeatures=features+oneHotFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define xgboost model\ndef make_predictions(tr_df, tt_df, features_columns, target, xgb_params, NFOLDS=2):\n    \n    folds = KFold(n_splits=NFOLDS)\n    \n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n    d_test = xgb.DMatrix(P)\n    \n    tt_df = tt_df[['Id',target]]    \n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros(len(tr_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n\n        tr_data = xgb.DMatrix(tr_x, label=tr_y)\n        vl_data = xgb.DMatrix(vl_x, label=vl_y)\n\n\n        estimator = xgb.train(\n            xgb_params,\n            tr_data,\n            10000,\n            evals = [(tr_data,'train'),(vl_data,'eval')],\n            early_stopping_rounds=500,\n            maximize=False, \n            verbose_eval = 1000,\n        )   \n        \n        pp_p = estimator.predict(d_test)\n        predictions += pp_p/NFOLDS\n        \n        oof_preds = estimator.predict(vl_data)\n        oof[val_idx] = oof_preds\n\n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    print('OOF RMSE:', sqrt(metrics.mean_squared_error(y,oof)))\n    #print('OOF RMSE:', sqrt(metrics.mean_squared_error(y.apply(lambda x: log(x)),np.log(oof))))\n    if LOCAL_TEST:\n        print('Holdout RMSE:', sqrt(metrics.mean_squared_error(tt_df[TARGET], tt_df['prediction'])))\n    \n    return (predictions,oof, estimator)\n\n\nxgb_params = {\n        'objective':'reg:linear',\n        'n_estimators': 10000,\n        'booster':'gbtree',\n        'max_depth':4,\n        'eval_metric':'mae',\n        'learning_rate':0.005, \n        'min_child_weight':2,\n        'subsample':0.7,\n        'colsample_bytree':0.8,\n        'seed':45,\n        'reg_alpha':1,\n        'gamma':0,\n        'nthread':-1\n\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run xgboost\ntest_predictions_xgb,oof_predictions_xgb,clf = make_predictions(data.iloc[trainIndex,:], data.iloc[testIndex,:], features, \"SalePrice\", xgb_params, NFOLDS=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get rid of na values, linear models can't use NA's\nsetMean=[\"LotFrontage\",'StreetToYard','KitchenQual','Functional',\"ElectricalFreq\"]                                                     \nfor feat in setMean:\n    data.loc[data[feat].isna(),feat]=np.mean(data[feat])\n\nsetZero=['MasVnrArea','BsmtFinSF1','BsmtUnfSF',\"BsmtFinSF2\",'TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\"GarageCars\",\"GarageArea\",\"TotalSF\"]                                               \nfor feat in setZero:\n    data.loc[data[feat].isna(),feat]=0\n\nsetMin=[\"GarageYrBlt\"]                                                    \nfor feat in setMin:\n    data.loc[data[feat].isna(),feat]=np.min(data[feat])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_rmse(model, X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=10))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define ridge regression model\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV,Ridge\ndef make_predictions(tr_df, tt_df, features_columns, target, xgb_params, NFOLDS=2):\n    \n    folds = KFold(n_splits=NFOLDS)\n    \n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n\n    \n    tt_df = tt_df[['Id',target]]    \n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros(len(tr_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n\n        tr_data = xgb.DMatrix(tr_x, label=tr_y)\n        vl_data = xgb.DMatrix(vl_x, label=vl_y)\n\n\n        clf = ridge = make_pipeline(RobustScaler(),\n                      Ridge()) #make_pipeline(StandardScaler(), SVR(gamma='auto'))\n        clf.fit(tr_x, tr_y)\n        \n        pp_p = clf.predict(P)\n        predictions += pp_p/NFOLDS\n        \n        oof_preds = clf.predict(vl_x)\n        oof[val_idx] = oof_preds\n\n\n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    print('OOF RMSE:', sqrt(metrics.mean_squared_error(y,oof)))\n    #print('OOF RMSE:', sqrt(metrics.mean_squared_error(y.apply(lambda x: log(x)),np.log(oof))))\n\n    return (predictions,oof, clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run ridge regression\ntest_predictions_Ridge,oof_predictions_Ridge,clf = make_predictions(data.iloc[trainIndex,:], data.iloc[testIndex,:], features, \"SalePrice\", xgb_params, NFOLDS=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This would be a good spot for the NN\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\ndef make_predictions(tr_df, tt_df, features_columns, target, NFOLDS=2):\n    \n    folds = KFold(n_splits=NFOLDS)\n    \n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n    \n    \n    tt_df = tt_df[['Id',target]]    \n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros(len(tr_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n\n        tr_data = xgb.DMatrix(tr_x, label=tr_y)\n        vl_data = xgb.DMatrix(vl_x, label=vl_y)\n        model = Sequential()\n        model.add(Dense(20,kernel_initializer='normal', activation='relu'))\n        model.add(Dense(25, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(10, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(5, kernel_initializer='normal', activation='relu'))\n        model.add(Dense(1, kernel_initializer='normal'))\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        model.fit(tr_x, tr_y, epochs=100, validation_split=0.2, verbose=0)\n        \n\n        \n        pp_p = model.predict(P)\n        predictions += pp_p/NFOLDS\n        \n        oof_preds = model.predict(vl_x)\n        oof[val_idx] = oof_preds\n\n\n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    print('OOF RMSE:', sqrt(metrics.mean_squared_error(y,oof)))\n    #print('OOF RMSE:', sqrt(metrics.mean_squared_error(y.apply(lambda x: log(x)),np.log(oof))))\n\n    return (predictions,oof, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#run neural network\ntest_predictions_NN,oof_predictions_NN,model = make_predictions(data.iloc[trainIndex,:], data.iloc[testIndex,:], features, \"SalePrice\", NFOLDS=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV,Ridge\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=10))\nscore = cv_rmse(ridge,data.iloc[trainIndex,:][features],data.iloc[trainIndex,:][\"SalePrice\"])\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV,Ridge\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=10))\nscore = cv_rmse(ridge,data.iloc[trainIndex,:][features],data.iloc[trainIndex,:][\"SalePrice\"])\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"},{"metadata":{"trusted":true},"cell_type":"code","source":"#blend values\nblended_oof=oof_predictions_lgbm*.6+oof_predictions_xgb*.15+oof_predictions_Ridge*.15\nblended_test=test_predictions_lgbm*.6+test_predictions_xgb*.15+test_predictions_Ridge*.15 + test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('OOF RMSE:', sqrt(metrics.mean_squared_error(target,oof_predictions_lgbm)))\nprint('OOF RMSE:', sqrt(metrics.mean_squared_error(target,oof_predictions_xgb)))\nprint('OOF RMSE:', sqrt(metrics.mean_squared_error(target,oof_predictions_Ridge)))\nprint('OOF RMSE:', sqrt(metrics.mean_squared_error(target,blended_oof)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#uncorrelates oof predictions means blends will be more powerful.\nnp.corrcoef(oof_predictions_svr,oof_predictions_lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission[\"SalePrice\"]=np.expm1(blended_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}